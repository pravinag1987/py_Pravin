import subprocess
import xml.etree.ElementTree as ET
import pandas as pd
import numpy as np
import os
from hdfs import InsecureClient

path = '/user/pravinag/python/Avg_Master_Feed_Pressure4.csv'

def run_cmd(args_list):
    print('Running system command: {0}'.format(' '.join(args_list)))
    proc = subprocess.Popen(args_list, stdout=subprocess.PIPE,
            stderr=subprocess.PIPE, shell=True)
    proc.communicate()
    return proc.returncode


# cmd = ['hadoop', 'fs', '-test', '-e', path]
# code = run_cmd(cmd)
# checkCMD = ['echo $?']
#
# check = run_cmd(checkCMD)
#
# if check == 1:
#     print ('file not exist')
#
# else:
#
#     print('File exist')


tree = ET.parse("/home/hduser1/PravinFiles/AtlasCapco/Master/Avg_Master_Feed_Pressure.xml")
root = tree.getroot()

get_range = lambda col: range(len(col))
l = [{r[i].tag:r[i].text for i in get_range(r)} for r in root]

df = pd.DataFrame.from_dict(l)


#file_name = '/home/hduser1/PravinFiles/AtlasCapco/Master/csv/Avg_Master_Feed_Pressure4.csv'

hdfs_path = '/user/pravinag/python/Avg_Master_Feed_Pressure4.csv'

client_hdfs = InsecureClient('http://localhost:50070')

with client_hdfs.write('/user/pravinag/python/Avg_Master_Feed_Pressure5.csv', encoding='utf-8') as writer:

    df[['AvgUnit', 'AvgUnit_Id']].dropna(how='all') \
        .to_csv(writer, index=False)